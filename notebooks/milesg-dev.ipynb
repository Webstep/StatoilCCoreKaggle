{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending \"/code/notebooks/..\" to path\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi=False\n",
    "\n",
    "import os; os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import ndimage\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "# Local imports\n",
    "import adjust_path  # Before doing any local imports\n",
    "from icc.data_loader import DataLoader\n",
    "from icc.models.gangsta_net import GangstaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = DataLoader.load_train()\n",
    "X['inc_angle_missing'] = X.inc_angle.map(pd.isnull)\n",
    "X.inc_angle.fillna(value=0, inplace=True)\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X.copy(), y.copy(), test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4089, 75, 75, 3)\n",
      "(4089,)\n"
     ]
    }
   ],
   "source": [
    "def transform(df):\n",
    "    images = []\n",
    "    for i, row in df.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 / band_2\n",
    "    \n",
    "        band_1_norm = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        band_2_norm = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        band_3_norm = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "        img = np.dstack((band_1_norm, band_2_norm, band_3_norm)) ** 2\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "def augment(images):\n",
    "    image_mirror_lr = []\n",
    "    image_mirror_ud = []\n",
    "    for i in range(0,images.shape[0]):\n",
    "        band_1 = images[i,:,:,0]\n",
    "        band_2 = images[i,:,:,1]\n",
    "        band_3 = images[i,:,:,2]\n",
    "            \n",
    "        # mirror left-right\n",
    "        band_1_mirror_lr = np.flip(band_1, 0)\n",
    "        band_2_mirror_lr = np.flip(band_2, 0)\n",
    "        band_3_mirror_lr = np.flip(band_3, 0)\n",
    "        image_mirror_lr.append(np.dstack((band_1_mirror_lr, band_2_mirror_lr, band_3_mirror_lr)))\n",
    "        \n",
    "        # mirror up-down\n",
    "        band_1_mirror_ud = np.flip(band_1, 1)\n",
    "        band_2_mirror_ud = np.flip(band_2, 1)\n",
    "        band_3_mirror_ud = np.flip(band_3, 1)\n",
    "        image_mirror_ud.append(np.dstack((band_1_mirror_ud, band_2_mirror_ud, band_3_mirror_ud)))\n",
    "        \n",
    "    mirrorlr = np.array(image_mirror_lr)\n",
    "    mirrorud = np.array(image_mirror_ud)\n",
    "    images = np.concatenate((images, mirrorlr, mirrorud))\n",
    "    return images\n",
    "\n",
    "xTrainImgs = transform(xTrain[['band_1', 'band_2']])\n",
    "xTestImgs = transform(xTest[['band_1', 'band_2']])\n",
    "\n",
    "xTrainImgs = augment(xTrainImgs)\n",
    "yTrain = np.concatenate((yTrain, yTrain, yTrain))\n",
    "print(xTrainImgs.shape)\n",
    "print(yTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoubleNet (\n",
       "  (avg_pool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  (max_pool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (input1_layer1_conv2d): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(3, 3))\n",
       "  (input1_layer2_conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (input1_layer3_conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (input1_layer4_fc): Linear (2592 -> 512)\n",
       "  (fc1): Linear (512 -> 256)\n",
       "  (fc2): Linear (256 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DoubleNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DoubleNet, self).__init__()\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        self.conv_out_shape = 2592\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Input 1 branch\n",
    "        self.input1_layer1_conv2d = nn.Conv2d(in_channels=3, out_channels=32,  kernel_size=5, padding=3)\n",
    "        self.input1_layer2_conv2d = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.input1_layer3_conv2d = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.input1_layer4_fc = nn.Linear(in_features=self.conv_out_shape, out_features=512)\n",
    "        \n",
    "        # Concatenated branch, fully connected stream\n",
    "        self.fc1 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=1)\n",
    "        \n",
    "    def forward(self, input1, training=False):\n",
    "        \n",
    "        input1 = self.max_pool(F.relu(self.input1_layer1_conv2d(input1)))\n",
    "        input1 = F.dropout2d(input1, p=0.2, training=training)\n",
    "        \n",
    "        input1 = self.max_pool(F.relu(self.input1_layer2_conv2d(input1)))\n",
    "        input1 = F.dropout2d(input1, p=0.2, training=training)\n",
    "        \n",
    "        input1 = self.max_pool(F.relu(self.input1_layer3_conv2d(input1)))\n",
    "        input1 = F.dropout2d(input1, p=0.2, training=training)\n",
    "        \n",
    "        input1 = F.relu(self.input1_layer4_fc(input1.view(-1, self.conv_out_shape)))\n",
    "        input1 = F.dropout2d(input1, p=0.2, training=training)\n",
    "        \n",
    "        combined = (self.fc1(input1))\n",
    "        combined = F.dropout2d(combined, p=0.2, training=training)\n",
    "        combined = self.fc2(combined)\n",
    "        \n",
    "        return combined\n",
    "\n",
    "DoubleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model\n",
      "defining optimizers\n",
      "starting training\n",
      "Epoch: 1, Train loss: 0.6268 - Test loss: 0.6160 - 100.0%\n",
      "Epoch: 2, Train loss: 0.6698 - Test loss: 0.5742 - 100.0%\n",
      "Epoch: 3, Train loss: 0.5632 - Test loss: 0.5754 - 100.0%\n",
      "Epoch: 4, Train loss: 0.5877 - Test loss: 0.5684 - 100.0%\n",
      "Epoch: 5, Train loss: 0.4747 - Test loss: 0.5513 - 100.0%\n",
      "Epoch: 6, Train loss: 0.5337 - Test loss: 0.5496 - 100.0%\n",
      "Epoch: 7, Train loss: 0.4857 - Test loss: 0.4773 - 100.0%\n",
      "Epoch: 8, Train loss: 0.5768 - Test loss: 0.4986 - 100.0%\n",
      "Epoch: 9, Train loss: 0.3977 - Test loss: 0.3635 - 100.0%\n",
      "Epoch: 10, Train loss: 0.5031 - Test loss: 0.3659 - 100.0%\n",
      "Epoch: 11, Train loss: 0.3246 - Test loss: 0.3672 - 100.0%\n",
      "Epoch: 12, Train loss: 0.4706 - Test loss: 0.3715 - 100.0%\n",
      "Epoch: 13, Train loss: 0.3843 - Test loss: 0.3384 - 100.0%\n",
      "Epoch: 14, Train loss: 0.2797 - Test loss: 0.3171 - 100.0%\n",
      "Epoch: 15, Train loss: 0.3131 - Test loss: 0.3204 - 100.0%\n",
      "Epoch: 16, Train loss: 0.2761 - Test loss: 0.3284 - 100.0%\n",
      "Epoch: 17, Train loss: 0.2296 - Test loss: 0.3462 - 100.0%\n",
      "Epoch: 18, Train loss: 0.3654 - Test loss: 0.3831 - 100.0%\n",
      "Epoch: 19, Train loss: 0.2390 - Test loss: 0.3166 - 100.0%\n",
      "Epoch: 20, Train loss: 0.2043 - Test loss: 0.2852 - 100.0%\n",
      "Epoch: 21, Train loss: 0.3821 - Test loss: 0.3043 - 100.0%\n",
      "Epoch: 22, Train loss: 0.2255 - Test loss: 0.2847 - 100.0%\n",
      "Epoch: 23, Train loss: 0.1559 - Test loss: 0.2934 - 100.0%\n",
      "Epoch: 24, Train loss: 0.1675 - Test loss: 0.3252 - 100.0%\n",
      "Epoch: 25, Train loss: 0.3366 - Test loss: 0.3051 - 100.0%\n",
      "Epoch: 26, Train loss: 0.5081 - Test loss: 0.3172 - 100.0%\n",
      "Epoch: 27, Train loss: 0.4638 - Test loss: 0.2840 - 100.0%\n",
      "Epoch: 28, Train loss: 0.3075 - Test loss: 0.2934 - 100.0%\n",
      "Epoch: 29, Train loss: 0.3855 - Test loss: 0.3759 - 100.0%\n",
      "Epoch: 30, Train loss: 0.2003 - Test loss: 0.3137 - 100.0%\n",
      "Epoch: 31, Train loss: 0.3594 - Test loss: 0.3136 - 100.0%\n",
      "Epoch: 32, Train loss: 0.1970 - Test loss: 0.3192 - 100.0%\n",
      "Epoch: 33, Train loss: 0.2272 - Test loss: 0.3117 - 100.0%\n",
      "Epoch: 34, Train loss: 0.1515 - Test loss: 0.2688 - 100.0%\n",
      "Epoch: 35, Train loss: 0.2836 - Test loss: 0.3268 - 100.0%\n"
     ]
    }
   ],
   "source": [
    "# print('transfering to cuda')\n",
    "print('initializing model')\n",
    "net = DoubleNet()\n",
    "net.cuda()\n",
    "\n",
    "n_epoch = 35\n",
    "batch_size = 100\n",
    "img_size = 75\n",
    "\n",
    "print('defining optimizers')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.0)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "datagen_params = dict(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-6,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.,\n",
    "    height_shift_range=0.,\n",
    "    shear_range=0.,\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='wrap',\n",
    "    cval=0.,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rescale=None,\n",
    "    preprocessing_function=None,\n",
    "    data_format='channels_last'\n",
    ")\n",
    "\n",
    "datagen = ImageDataGenerator(**datagen_params)\n",
    "datagen.fit(xTrainImgs, augment=True, seed=123)\n",
    "\n",
    "# Test data setup\n",
    "testImgs = Variable(torch.FloatTensor(xTestImgs.reshape(-1, 3, 75, 75)).cuda())\n",
    "testTarget = Variable(torch.FloatTensor(yTest.values.astype(float).reshape(-1, 1)).cuda(), requires_grad=False)\n",
    "\n",
    "\n",
    "print('starting training')\n",
    "for epoch in range(n_epoch):\n",
    "    i = 0\n",
    "    for img_batch, target in datagen.flow(xTrainImgs, yTrain):\n",
    "        \n",
    "        trainImgs = Variable(torch.FloatTensor(img_batch.reshape(-1, 3, 75, 75)).cuda())\n",
    "        target = Variable(torch.FloatTensor(target.astype(float).reshape(-1, 1)).cuda())\n",
    "\n",
    "        # batch step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = net(trainImgs, training=True)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        i += 1\n",
    "        if i % 5 == 0: \n",
    "            \n",
    "            out = net(testImgs)\n",
    "            train_loss = criterion(out, testTarget).data.cpu().numpy()[0]\n",
    "            pct_complete = (i / 100) * 100\n",
    "            sys.stdout.write('\\rEpoch: {}, Train loss: {:.4f} - Test loss: {:.4f} - {:.1f}%'\n",
    "                             .format(epoch+1, loss.data.cpu().numpy()[0], train_loss, pct_complete))\n",
    "        if i > 100:\n",
    "            sys.stdout.write('\\rEpoch: {}, Train loss: {:.4f} - Test loss: {:.4f} - {:.1f}%'\n",
    "                             .format(epoch+1, loss.data.cpu().numpy()[0], train_loss, 100))\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 9, 9])\n",
      "2592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-8.3421e-03 -1.1375e-02 -5.7629e-02  ...  -3.6862e-02  1.4998e-02  4.7157e-02\n",
       "-1.8848e-02 -7.1759e-03 -6.2996e-02  ...  -4.3536e-02  8.2824e-03  3.9818e-02\n",
       "[torch.FloatTensor of size 2x512]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# network layer trials\n",
    "img_size = 75\n",
    "\n",
    "input1_layer1_conv2d = nn.Conv2d(in_channels=3, out_channels=32,  kernel_size=5, padding=3)\n",
    "input1_layer2_conv2d = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "input1_layer3_conv2d = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "fc1 = nn.Linear(in_features=2592, out_features=512)\n",
    "pool_big = nn.MaxPool2d(kernel_size=3, stride=2, )\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2, )\n",
    "\n",
    "pics = np.concatenate(xTrainImgs[:2], axis=0).reshape(-1, 3, img_size, img_size)\n",
    "pics = torch.FloatTensor(pics)\n",
    "pics = Variable(pics)\n",
    "\n",
    "pics = pool(input1_layer1_conv2d(pics))\n",
    "pics = pool(input1_layer2_conv2d(pics))\n",
    "pics = pool(input1_layer3_conv2d(pics))\n",
    "\n",
    "print(pics.size())\n",
    "print(np.prod(pics.size()[1:]))\n",
    "\n",
    "fc1(pics.view(-1, 2592))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
