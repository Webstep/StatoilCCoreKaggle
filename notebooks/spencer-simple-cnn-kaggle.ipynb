{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.kaggle.com/jirivrany/my-best-single-model-simple-cnn-lb-0-1541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import cv2 # Used to manipulated the images \n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Kfold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Import Keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Local imports\n",
    "import adjust_path  # Before doing any local imports\n",
    "from icc.data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_imgs(df):\n",
    "    \"\"\"\n",
    "    basic function for reshaping and rescaling data as images\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "        \n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))\n",
    "\n",
    "    return np.array(imgs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_images(imgs):\n",
    "    \"\"\"\n",
    "    augmentation for more data\n",
    "    \"\"\"    \n",
    "\n",
    "\n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "      \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        a=imgs[i,:,:,0]\n",
    "        b=imgs[i,:,:,1]\n",
    "        c=imgs[i,:,:,2]\n",
    "        \n",
    "        av=cv2.flip(a,1)\n",
    "        ah=cv2.flip(a,0)\n",
    "        bv=cv2.flip(b,1)\n",
    "        bh=cv2.flip(b,0)\n",
    "        cv=cv2.flip(c,1)\n",
    "        ch=cv2.flip(c,0)\n",
    "        \n",
    "        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n",
    "        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n",
    "      \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,v,h))\n",
    "    \n",
    "    return more_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    \"\"\"\n",
    "    Keras Sequential model\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    model=Sequential()\n",
    "    \n",
    "    # Conv block 1\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "   \n",
    "    # Conv block 2\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "   \n",
    "    # Conv block 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "   \n",
    "    #Conv block 4\n",
    "    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "   \n",
    "    # Flatten before dense\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #Dense 1\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    #Dense 2\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizer = Adam(lr=0.0001, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = DataLoader.load_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = get_scaled_imgs(X)\n",
    "Ytrain = np.array(y)\n",
    "X.inc_angle = X.inc_angle.replace('na',0)\n",
    "idx_tr = np.where(X.inc_angle>0) # about 1471 images greater than angle 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = Ytrain[idx_tr[0]]\n",
    "Xtrain = Xtrain[idx_tr[0],...]\n",
    "\n",
    "Xtr_more = get_more_images(Xtrain) \n",
    "Ytr_more = np.concatenate((Ytrain,Ytrain,Ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_more.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K fold CV training\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "for fold_n, (train, test) in enumerate(kfold.split(Xtr_more, Ytr_more)):\n",
    "    print(\"FOLD number: \", fold_n)\n",
    "    model = get_model()\n",
    "    \n",
    "    #MODEL_FILE = 'mdl_simple_k{}_wght.hdf5'.format(fold_n)\n",
    "    batch_size = 32\n",
    "    #mcp_save = ModelCheckpoint(MODEL_FILE, save_best_only=True, monitor='val_loss', mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "    # set the epochs to 30 before training on your GPU\n",
    "    model.fit(Xtr_more[train], Ytr_more[train],\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        verbose=1,\n",
    "        validation_data=(Xtr_more[test], Ytr_more[test]),\n",
    "        callbacks=[reduce_lr_loss])\n",
    "        #callbacks=[mcp_save, reduce_lr_loss])\n",
    "    \n",
    "    #model.load_weights(filepath = MODEL_FILE)\n",
    "\n",
    "    score = model.evaluate(Xtr_more[test], Ytr_more[test], verbose=1)\n",
    "    print('\\n Val score:', score[0])\n",
    "    print('\\n Val accuracy:', score[1])\n",
    "\n",
    "    SUBMISSION = './saved_model/simplenet/sub_simple_v1_{}.csv'.format(fold_n)\n",
    "\n",
    "    df_test = DataLoader.load_test()\n",
    "    #df_test = pd.read_json('../input/test.json')\n",
    "    df_test.inc_angle = df_test.inc_angle.replace('na',0)\n",
    "    Xtest = (get_scaled_imgs(df_test))\n",
    "    pred_test = model.predict(Xtest)\n",
    "\n",
    "    submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': pred_test.reshape((pred_test.shape[0]))})\n",
    "    print(submission.head(10))\n",
    "\n",
    "    submission.to_csv(SUBMISSION, index=False)\n",
    "    print(\"submission saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = './saved_model/simplenet/'\n",
    "stacked_1 = pd.read_csv(wdir + 'sub_simple_v1_0.csv')\n",
    "stacked_2 = pd.read_csv(wdir + 'sub_simple_v1_1.csv')\n",
    "stacked_3 = pd.read_csv(wdir + 'sub_simple_v1_2.csv')\n",
    "stacked_4 = pd.read_csv(wdir + 'sub_simple_v1_3.csv')\n",
    "stacked_5 = pd.read_csv(wdir + 'sub_simple_v1_4.csv')\n",
    "stacked_6 = pd.read_csv(wdir + 'sub_simple_v1_5.csv')\n",
    "stacked_7 = pd.read_csv(wdir + 'sub_simple_v1_6.csv')\n",
    "stacked_8 = pd.read_csv(wdir + 'sub_simple_v1_7.csv')\n",
    "stacked_9 = pd.read_csv(wdir + 'sub_simple_v1_8.csv')\n",
    "stacked_10 = pd.read_csv(wdir + 'sub_simple_v1_9.csv')\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = stacked_1['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(stacked_1['is_iceberg'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_1['is_iceberg'].apply(lambda x: np.log(x)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['is_iceberg'] = np.exp(np.mean(\n",
    "    [stacked_1['is_iceberg'].apply(lambda x: np.log(x)),\n",
    "    stacked_2['is_iceberg'].apply(lambda x: np.log(x)),\n",
    "    stacked_3['is_iceberg'].apply(lambda x: np.log(x)),\n",
    "    stacked_4['is_iceberg'].apply(lambda x: np.log(x)),\n",
    "    stacked_5['is_iceberg'].apply(lambda x: np.log(x)),\n",
    "    stacked_6['is_iceberg'].apply(lambda x: np.log(x)),\n",
    "    stacked_7['is_iceberg'].apply(lambda x: np.log(x)),\n",
    "    stacked_8['is_iceberg'].apply(lambda x: np.log(x)),\n",
    "    stacked_9['is_iceberg'].apply(lambda x: np.log(x)),\n",
    "    stacked_10['is_iceberg'].apply(lambda x: np.log(x)),\n",
    "    ], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('final_ensemble.csv', index=False, float_format='%.6f')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.kaggle.com/dongxu027/explore-stacking-lb-0-1463"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub = pd.concat([stacked_1,\n",
    "                        stacked_2['is_iceberg'], \n",
    "                        stacked_3['is_iceberg'], \n",
    "                        stacked_4['is_iceberg'],\n",
    "                        stacked_5['is_iceberg']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(map(lambda x: \"is_iceberg_\" + str(x), range(len(concat_sub.columns[1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub.columns = ['id'] + cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "concat_sub.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data fields ready for stacking\n",
    "concat_sub['is_iceberg_max'] = concat_sub.iloc[:, 1:6].max(axis=1) # for each sample, pick its maximum value across all the predictions\n",
    "concat_sub['is_iceberg_min'] = concat_sub.iloc[:, 1:6].min(axis=1)\n",
    "concat_sub['is_iceberg_mean'] = concat_sub.iloc[:, 1:6].mean(axis=1)\n",
    "concat_sub['is_iceberg_median'] = concat_sub.iloc[:, 1:6].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for submission stacking:\n",
    "    \n",
    "    - Mean stacking\n",
    "    - Median stacking\n",
    "    - PushOut and Median Stacking: middle predictions and only submit extremes\n",
    "    - MinMax and Mean Stacking\n",
    "    - MinMax and Median Stacking\n",
    "    - MinMax and BestBase stacking\n",
    "    \n",
    "Tips:\n",
    "\n",
    "    - Building a strong and robust model is always a key component\n",
    "    - stacking only comes last with the intention to perform a little better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation:\n",
    "    - Best way to use it as a benchmark to avoid overfitting on the LB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take my personal best 5 submissions and then either perform on the top stacking methods, ie. MinMax + BestBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "    https://github.com/QuantScientist/Deep-Learning-Boot-Camp/blob/master/Kaggle-PyTorch/README.md\n",
    "    https://www.kaggle.com/solomonk/statoil-csv-pytorch-senet-ensemble-lb-0-1520\n",
    "\n",
    "Ensemble ~50 ResNets\n",
    "\n",
    "More about ensembling:\n",
    "    \n",
    "    https://mlwave.com/kaggle-ensembling-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other types of CNN:\n",
    "\n",
    "Residual Network\n",
    "\n",
    "    [Deep Residual Learning for Image Recognition][5]\n",
    "    [Identity Mappings in Deep Residual Networks][6]\n",
    "\n",
    "ResNeXt\n",
    "\n",
    "    [Aggregated Residual Transformations for Deep Neural Networks][8]\n",
    "\n",
    "DenseNet\n",
    "\n",
    "    [Densely Connected Convolutional Networks][9]\n",
    "\n",
    "SENet\n",
    "\n",
    "    [Squeeze-and-Excitation Networks][10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
